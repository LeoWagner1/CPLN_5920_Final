---
title: "Predicting Gentrification"
author: "Tao Chen, Riya Saini, Leo Wagner"
date: "05-13-2024"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    code_download: true
    theme: journal  
editor_options: 
  markdown: 
    wrap: 72
---

# Notes

[Fichman & co's
paper](https://urbanspatialanalysis.com/portfolio/predicting-gentrification-using-longitudinal-census-data/)
centers around housing price as the proxy for predicting change. They
use 1990 and 2000 Census data on home prices to predict home prices in
2010. If those models prove robust, we can use the model to forecast for
2020.

Pew's 3 criteria from their [2016
report](https://www.pewtrusts.org/en/research-and-analysis/reports/2016/05/philadelphias-changing-neighborhoods),
tracking gentrification in Philadelphia between 2010 and 2014:

-   In order to be considered gentrified, a census tract needed a median
    household income in 2000 below 80 percent of regional median income,
    53,992 dollars, the threshold set by the federal government to
    determine eligibility for housing assistance and other programs
    aimed at low-income households
-   The tract’s median income had to have increased at least 10 percent
    in inflation-adjusted dollars from 2000 to 2014, a period in which
    the median income of the city as a whole actually fell by about 10
    percent
-   Its 2014 median household income figure had to exceed the citywide
    median of 37,460 dollars.

# Introduction

We use a similar metric to that used by Fichman et al., primarily using
median house value and its geospatial metrics, while taking into account
other factors along the way.

We use 2010 and 2014 Philadelphia data to predict house prices in 2018.
Then the model is tested on Baltimore. These years are chosen to ensure
that the census tracts stay the same, making comparisons easier. Of
course, if a good model is found, other years can be compared.

We use 2010 prices as a standard to account for inflation.

# Set Up

```{r library and data, include=FALSE}
library(tidyverse)
library(sf)
library(lubridate)
library(tigris)
library(gganimate)
library(riem)
library(gridExtra)
library(knitr)
library(kableExtra)
library(dplyr)
library(tidycensus)
library(mapview)
library(sfdep)
library(spdep)
library(caret)
library (readr)
library(riem)
library(stargazer)
library(purrr)
library(magick)
library(viridis)
library(corrr)
# Use this in console if needed: 
# install.packages("magick", repos = "http://cran.us.r-project.org")

options(tigris_class = "sf")

palette5 <- c("#124E78","#F0F0C9","#F2BB05","#D74E09","#6E0E0A")
palette4 <- c("#124E78","#F0F0C9","#F2BB05","#6E0E0A")
palette3 <- c("#124E78","#F2BB05","#6E0E0A")
palette2 <- c("#124E78","#6E0E0A")

tidycensus::census_api_key("ac71174eb1882d2aea25c533ed9fdb342343bf4a", overwrite = TRUE)
v10 <- load_variables(2010, "acs5", cache = TRUE)
```

# Data Collection

Calculation for inflation is done using the [Bureau of Labor Statistics'
resources](https://data.bls.gov/cgi-bin/cpicalc.pl?cost1=10.00&year1=201001&year2=201501)

## Philadelphia Data

```{r data acquiring}
# Define function to retrieve and process ACS data
get_processed_acs <- function(year, state, county, city_name) {
  variables <- c(
    Total_Pop = "B01003_001",
    White_Pop = "B01001A_001",
    BachelorsDegree = "B06009_005",
    Med_HH_Inc = "B19013_001",
    Med_House_Val = "B25077_001",
    Med_Rent = "B25064_001"
  )
  
  # Retrieve ACS data
  city_data <- get_acs(
    geography = "tract",
    variables = variables,
    year = year,
    state = state,
    county = county,
    geometry = TRUE,
    output = "wide"
  )
  
  # Process and transform the data
  city_data <- city_data %>%
    mutate(NAME = sub(",.*$", "", NAME)) %>%
    dplyr::select(-ends_with("M")) %>%
    rename_with(~ gsub("E$", "", .), ends_with("E")) %>%
    rename(Tract = NAM) %>%
    mutate(White_Share = (White_Pop / Total_Pop) * 100)
  
  if (year == 2014) {
    city_data <- city_data %>%
      mutate(
        Med_HH_Inc = Med_HH_Inc / 1.08,   # Adjust for Inflation 
        Med_House_Val = Med_House_Val / 1.08,
        Med_Rent = Med_Rent / 1.08
      )
  } else if (year == 2018) {
    city_data <- city_data %>%
      mutate(
        Med_HH_Inc = Med_HH_Inc / 1.1144,  # Adjust for Inflation 
        Med_House_Val = Med_House_Val / 1.1144,
        Med_Rent = Med_Rent / 1.1144
      )
  }
  
  return(city_data)
}

# Retrieve and process ACS data for Philadelphia
Philly2010 <- get_processed_acs(2010, "PA", "Philadelphia", "Philadelphia")
Philly2014 <- get_processed_acs(2014, "PA", "Philadelphia", "Philadelphia")
Philly2018 <- get_processed_acs(2018, "PA", "Philadelphia", "Philadelphia")

# Retrieve and process ACS data for Baltimore
Balti2010 <- get_processed_acs(2010, "MD", "Baltimore City", "Baltimore")
Balti2014 <- get_processed_acs(2014, "MD", "Baltimore City", "Baltimore")
Balti2018 <- get_processed_acs(2018, "MD", "Baltimore City", "Baltimore")

Philly_combined <- rbind(transform(Philly2010, year = 2010), 
                         transform(Philly2014, year = 2014))

```

# Quick Look

Discuss what we're doing here

## Median House Value

Discuss what this is going to show -- primary component of Fichman et
al. paper

```{r map 1, warning=FALSE}

ggplot(Philly_combined, aes(fill = Med_House_Val)) +
  geom_sf() +
  scale_fill_viridis_c(labels = scales::label_number(), 
                       name = "Median House Value",
                       na.value = "#ebebeb",
                       guide = guide_colorbar(title.position = "top", title.hjust = 0.5,
                                              title = "In 2010 Dollars")) +  
  labs(title = "Median House Value in Philadelphia",
       fill = NULL) +  # Remove the fill legend title
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.subtitle = element_text(size = 9, face = "italic"),
        plot.title = element_text(size = 12, face = "bold"),
        panel.background = element_blank(),
        panel.border = element_rect(colour = "#F5f5f5", fill = NA, linewidth = 0.8)) +
  facet_wrap(~year, nrow = 1)

```

Discuss additional nuance examining price distribution is going to add

```{r housing price distribution, warning=FALSE}
Philly_housingprice <- rbind(transform(Philly2010, year = 2010), 
                         transform(Philly2014, year = 2014),
                         transform(Philly2018, year = 2018))

ggplot(Philly_housingprice, aes(x = factor(year), y = Med_House_Val, fill = factor(year))) +
  geom_violin() +
  scale_fill_manual(values = palette3) +
  labs(x = "Year", y = "House Value (thousands $)", title = "House Value Distribution in Philadelphia") +
  scale_y_continuous(labels = function(x) paste0(x/1000, "K")) +
  theme_minimal() + 
  stat_summary(fun.y = median, geom = "point", size = 2, color = "white")
```

Discuss findings While the median has not changed much, note the spike
in the peak home value in 2018. The area around 100,000 dollars also
thinned out significantly.

## White Population Percentage

Why we include this--incl. article link potentially to show significance

```{r map 1}
ggplot(Philly_combined, aes(fill = White_Share)) +
  geom_sf() +
  scale_fill_viridis_c(labels = scales::label_number(), 
                       name = "White_Pop_Pct",
                       na.value = "#ebebeb",
                       guide = guide_colorbar(title.position = "top", title.hjust = 0.5,
                                              title = "Percentage")) +  
  labs(title = "White Population Percentage in Philadelphia",
       fill = NULL) +  # Remove the fill legend title
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.subtitle = element_text(size = 9, face = "italic"),
        plot.title = element_text(size = 12, face = "bold"),
        panel.background = element_blank(),
        panel.border = element_rect(colour = "#F5f5f5", fill = NA, linewidth = 0.8)) +
  facet_wrap(~year, nrow = 1)

```

Discuss findings--decline in % in some tracts in the Northeast, growth
in % in Grays Ferry/Point Breeze, lower North Philadelphia

## Philadelphia 2010-2014 Changes

```{r PhillyChange Table 1}
Philly10to14 <- Philly_combined %>%
  group_by(GEOID, Tract) %>%
  summarise(
    Total_Pop_change = Total_Pop[year == 2014] - Total_Pop[year == 2010],
    Total_White_Pop_change = White_Pop[year == 2014] - White_Pop[year == 2010],
    BachelorsDegree_change = BachelorsDegree[year == 2014] - BachelorsDegree[year == 2010],
    Med_HH_Inc_change = Med_HH_Inc[year == 2014] - Med_HH_Inc[year == 2010],
    Med_House_Val_change = Med_House_Val[year == 2014] - Med_House_Val[year == 2010],
    Med_Rent_change = Med_Rent[year == 2014] - Med_Rent[year == 2010],
    White_Share_Change = White_Share[year == 2014] - White_Share[year == 2010]
  )
Philly2010NoGeo <- Philly2010 %>%
  st_drop_geometry()

Philly10to14 <- left_join(Philly10to14, Philly2010NoGeo, by = "Tract") %>%
  dplyr::select(-GEOID.y)

Philly10to14 <- Philly10to14 %>%
  mutate(
    Pop_PctChange = (Total_Pop_change/Total_Pop)*100,
    White_Pop_PctChange = (Total_White_Pop_change/White_Pop)*100,
    BachelorsDegree_PctChange = (BachelorsDegree_change/BachelorsDegree)*100,
    Med_HH_Inc_PctChange = (Med_HH_Inc_change/Med_HH_Inc)*100,
    Med_House_Val_PctChange = (Med_House_Val_change/Med_House_Val)*100,
    Med_Rent_PctChange = (Med_Rent_change/Med_Rent)*100
  ) %>%
  dplyr::select(-Total_Pop,
         -White_Pop,
         -BachelorsDegree,
         -Med_HH_Inc,
         -Med_House_Val,
         -Med_Rent_change,
         -White_Share)
```

Connect median house value to home value *change* and how this is
significant

```{r Philly Change Map 1}
ggplot(Philly10to14, aes(fill = Med_House_Val_PctChange)) +
  geom_sf() +
  scale_fill_gradient2(low = "#2C7BB6", mid = "white", high = "#D7191C",
                       midpoint = 0,
                       labels = scales::label_number(), 
                       name = "Median House Value Change",
                       guide = guide_colorbar(title.position = "top", title.hjust = 0.5,
                                              title = "Percentage Change")) +  
  labs(title = "Median House Value Change in Philadelphia 2010-2014",
       fill = NULL) +  # Remove the fill legend title
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank(),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        plot.subtitle = element_text(size = 9, face = "italic"),
        plot.title = element_text(size = 12, face = "bold"),
        panel.background = element_blank(),
        panel.border = element_rect(colour = "#F5f5f5", fill = NA, linewidth = 0.8))

```

Discuss results, when compared with median house values and white
population in 2010 and 2014 areas with extremely cheap houses and very
low percentage of white residents saw the greatest *percentage* increase
in house values--largely north-central Philadelphia in areas close to
existing "creative class" Center City-adjacent areas, such as Fairmount,
with some affects of the widening sphere of influence of Temple U as
well. (partially explained by law of diminishing marginal returns)

# Data Wrangling

## Spatial Lag

The following deals with median house value. In 2010's data, there are
only 366 obs as opposed to 384 obs. A few tracts do not have median home
values. This will appear as "Philly2010Lag"; 2014 data will be joined as
"Philly2010Lag14".

```{r data wrangling spatial lag}
# Replace non-finite values with NA
Philly2010$Med_House_Val[!is.finite(Philly2010$Med_House_Val)] <- NA #Rows without values are omitted here

# Remove NA rows for calculating the spatial lag
Philly2010Lag <- na.omit(Philly2010) 

# Create neighbor list weights
Philly2010nb <- poly2nb(Philly2010Lag, queen = TRUE)
Philly2010weights <- nb2listw(Philly2010nb, style = "W")

# Calculate the spatial lag
Philly2010Lag$NborHouseVal <- lag.listw(Philly2010weights, Philly2010Lag$Med_House_Val)

# Now to see the correlation between 2010 Spatial Lag values (average neighbor tracts house value) and 2014 House Prices
Philly2014NoGeo <- Philly2014 %>%
  st_drop_geometry()

Philly2010NoGeo <- Philly2010Lag %>%
  st_drop_geometry()

Philly2010Lag14 <- merge(Philly2010NoGeo, Philly2014NoGeo, by = "GEOID", suffixes = c("_2010", "_2014"))

# Calculate the correlation
correlation <- cor(Philly2010Lag14$NborHouseVal, Philly2010Lag14$Med_House_Val_2014)

#Moran's I Coefficient
Lag1 <- lm(Philly2010Lag14$NborHouseVal ~ Philly2010Lag14$Med_House_Val_2014)
coef(Lag1)[2]

# Create a scatter plot
ggplot(Philly2010Lag14, aes(x = NborHouseVal, y = Med_House_Val_2014)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +  # Add linear regression line
  labs(title = "2010 Average Neighbor Tracts Home Value vs 2014 Home Value",
       x = "Spatial Lag of Median Home Value (2010)",
       y = "Median Home Value (2014)") +
  scale_x_continuous(labels = function(x) paste0(x/1000, "K")) +  # Format x-axis labels as numeric in thousands
  scale_y_continuous(labels = function(x) paste0(x/1000, "K")) +  # Format y-axis labels as numeric in thousands
  theme_minimal()

```

The Moran’s I coefficient is 0.74. The positive (upward) slope suggests
that as the house value of a said polygon increases, so does those of
its neighboring polygons.

## Moran's I

(The following is taken from Fichman & Co's page) There are three local
spatial patterns of home prices that we are interested in: clustering of
high prices; clustering of low prices; and spatial randomness of prices.

Local clustering of high and low prices suggests that housing market
agents agree on equilibrium prices in a given neighborhood. Local
randomness, we argue, is indicative of a changing neighborhood – one
that is out of equilibrium.

In a changing neighborhood, buyers and sellers are unable to predict the
value of future amenities. Our theory argues that when this uncertainty
is capitalized into prices, the result is a heterogeneous pattern of
prices across space. Capturing this spatial trend is crucial for
forecasting neighborhood change.

--LEO: also indicative of many properties with distinct amenities (e.g.,
those of shoddier, older construction, and recently-renovated and
brand-new construction buildings), or of hyperlocal variation within a
Census tract (i.e. some portions are attractive to wealthier prospective
buyers and renters, but not all). Both these patterns indicative of
displacement

Homogeneous prices in the are indicative of an area in equilibrium where
all housing market agents agree on future expectations. Heterogeneous
prices is more indicative of a neighborhood in flux – one where housing
market agents are capitalizing an uncertain future into prices.

```{r local moran's}
## {spdep} to make polygon to neighborhoods... 
Phil2010.nb <- poly2nb(as_Spatial(Philly2010Lag), queen=TRUE)
## ... and neighborhoods to list of weigths
Phil2010.weights <- nb2listw(Phil2010.nb, style="W", zero.policy=TRUE)

Phil2010MoranResult <- localmoran(Philly2010Lag$Med_House_Val, Phil2010.weights, zero.policy=TRUE) %>% 
  as.data.frame()


Philly2010.LMoransList <-
  cbind(Phil2010MoranResult, as.data.frame(Philly2010Lag)) %>%
  st_sf() %>%
  dplyr::select(Med_House_Val = Med_House_Val, 
                GEOID, # so joining will be easier later, for the P-Value
                Local_Morans_I = Ii, 
                P_Value = `Pr(z != E(Ii))`) %>%
  mutate(Significant_Hotspots = ifelse((P_Value <= 0.05), 1, 0))

 # for plotting use in the next stage, without GEOID. GEOID messes with the process somehow
Philly2010.LMoransList1 <-
  cbind(Phil2010MoranResult, as.data.frame(Philly2010Lag)) %>%
  st_sf() %>%
  dplyr::select(Med_House_Val = Med_House_Val, 
                Local_Morans_I = Ii, 
                P_Value = `Pr(z != E(Ii))`) %>%
  mutate(Significant_Hotspots = ifelse((P_Value <= 0.05), 1, 0))

Philly2010.LocalMorans <- Philly2010.LMoransList%>% 
  gather(Variable, Value, -geometry)

Philly2010.LocalMorans1 <- Philly2010.LMoransList1%>% 
  gather(Variable, Value, -geometry)

```

```{r plotting local moran's}
## This is just for plotting
Philly2010MoransPlot <- unique(Philly2010.LocalMorans1$Variable)
PhillyMoransList <- list()

for(i in Philly2010MoransPlot){
  PhillyMoransList[[i]] <- 
    ggplot() +
    geom_sf(data = filter(Philly2010.LocalMorans1, Variable == i), 
            aes(fill = Value), colour=NA) +
    scale_fill_viridis(name="") +  # Format labels with numerals
    labs(title=i) +
    theme_void() +
    theme(legend.position="bottom")
}

do.call(grid.arrange,c(PhillyMoransList, ncol = 4, top = "Local Morans I statistics, Philadelphia 2010 Median Home Value"))
```

Interpretation of chart elements: see Assignment 4

Moran's I correlation--why this is included

```{r moran's correlation}
Philly2010Lag14 <- left_join(Philly2010Lag14, Philly2010.LMoransList, by = "GEOID")

Philly2010Lag14 <- Philly2010Lag14 %>%
  dplyr::select(-Med_House_Val, -Significant_Hotspots)

# Create a scatter plot
ggplot(Philly2010Lag14, aes(x = P_Value, y = Med_House_Val_2014)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +  # Add linear regression line
  labs(title = "2014 Price as a function of the 2010 Local Moran's I Value",
       x = "Local Moran's I P-Value - Median Home Value (2010)",
       y = "Median Home Value (2014)") +
  scale_x_continuous(labels = scales::label_number()) +  # Format x-axis labels as numeric in thousands
  scale_y_continuous(labels = function(x) paste0(x/1000, "K")) +  # Format y-axis labels as numeric in thousands
  theme_minimal()
```

results of this correlation

Before developing the Ordinary Least Squared Regression model, lets look
at the Correlation Matrix. (talk about correlation matrix- values close
to 1 and -1 should be omitted).

```{r correlation matrix, fig.height=12, fig.width=12}

Philly1014 <- Philly2010Lag14 %>%
  mutate(
    Total_Pop_change = Total_Pop_2014 - Total_Pop_2010,
    Total_White_Pop_change = White_Pop_2014 - White_Pop_2010,
    BachelorsDegree_change = BachelorsDegree_2014 - BachelorsDegree_2010,
    Med_HH_Inc_change = Med_HH_Inc_2014 - Med_HH_Inc_2010,
    Med_House_Val_change = Med_House_Val_2014 - Med_House_Val_2010,
    Med_Rent_change = Med_Rent_2014 - Med_Rent_2010,
    White_Share_Change = White_Share_2014 - White_Share_2010)%>%
  na.omit()


Philly2018_df <- as.data.frame(Philly2018)
dat18 <- merge(Philly1014, Philly2018_df, by = "GEOID", all.x = FALSE, all.y=FALSE, sort 
= FALSE)

# correlation matrix
dat18.Numeric <- dat18 %>%
  st_drop_geometry() %>%  
  select_if(is.numeric) %>%  
  na.omit()  
dat18.Numeric <- dat18 %>% select_if(is.numeric)
# Calculate correlation matrix
correlation_matrix <- cor(dat18.Numeric)

# Create correlation plot
dat18.Numeric %>% 
  correlate() %>% 
  autoplot() +
  geom_text(aes(label = round(r,digits=2)),size = 1)
```

OLS Regression The purpose of Linear Regression or Ordinary Least
Squares Regression (OLS), is to predict the Median Household Value of
house in 2018, as a function of several components. A step-wise
regression model based on addition and deletion of variables based on
its p-value and contribution to the model's predictive power has been
created. The goodness of fit of the model was ascertained using measures
such as the Variance inflation factor, AIC and BIC. For the two most
competitive regression models, an anova test or Analysis of Variance was
conducted to test the overall significance of a regression model. In
this case, Regression Model 2 has a significantly lower RSS (Residual
Sum of Squares) compared to Model 1, which is reflected in the large
difference in the sum of squares between the two models. Additionally,
Model 2 has a significant F-value, with a p-value close to 0, indicating
that the model is statistically significant.

```{r regression, message=FALSE, warning=FALSE, echo=FALSE}
# reg1, AIC, BIC, VIF, ANOVA can be deleted if need be. 
reg1 <- lm(Med_House_Val ~ . - Total_Pop_2010 - White_Pop_2010 - BachelorsDegree_2010 - 
                           Med_HH_Inc_2010 - Med_House_Val_2010 - Med_Rent_2010 - White_Share_2010 - 
                           Total_Pop_2014 - White_Pop_2014 - BachelorsDegree_2014 - Med_HH_Inc_2014 - 
                           Med_Rent_2014 - White_Share_2014 - Local_Morans_I - P_Value -White_Share -White_Pop -Med_House_Val_2014, data = dat18.Numeric)

library(car)
vif(reg1)
AIC(reg1)
BIC(reg1)

reg2 <- lm(Med_House_Val ~ . - Total_Pop_2010 - White_Pop_2010 - BachelorsDegree_2010 - 
                           Med_HH_Inc_2010 - Med_House_Val_2010 - Med_Rent_2010 - White_Share_2010 - 
                           Total_Pop_2014 - White_Pop_2014 - BachelorsDegree_2014 - Med_HH_Inc_2014 - 
                           Med_Rent_2014 - White_Share_2014 - Local_Morans_I - P_Value -White_Share -White_Pop -White_Share_Change -Total_Pop_change, data = dat18.Numeric)
options(scipen = 999)
stargazer(reg2, type = "text")
vif(reg2)
AIC(reg2)
BIC(reg2)

anova(reg1, reg2)
```

K-fold Cross Validation

```{r}
set.seed(123)
dat18.Numeric <- na.omit(dat18.Numeric)

#Define the control parameters for k-fold cross-validation
control <- trainControl(method = "cv",
                        number = 10,      
                        verboseIter = TRUE,  
                        returnData = FALSE,  
                        savePredictions = TRUE, 
                        classProbs = FALSE,  
                        summaryFunction = defaultSummary)  

# Train the linear regression model using k-fold cross-validation
lm_cv <- train(Med_House_Val ~ . - Total_Pop_2010 - White_Pop_2010 - BachelorsDegree_2010 - 
                           Med_HH_Inc_2010 - Med_House_Val_2010 - Med_Rent_2010 - White_Share_2010 - 
                           Total_Pop_2014 - White_Pop_2014 - BachelorsDegree_2014 - Med_HH_Inc_2014 - 
                           Med_Rent_2014 - White_Share_2014 - Local_Morans_I - P_Value -White_Share -White_Pop -White_Share_Change -Total_Pop_change,  
               data = dat18.Numeric,  
               method = "lm",          
               trControl = control)     

print(lm_cv)
```

# Mean Absolute Error

To find out goodness of fit, Mean Error for each fold across each
regression is derived by subtracting predicted values from observed
Median House Values in 2018. Subsequently, Mean Absolute Error and
Standard deviation of MAE are added. The graph identifies census tracts
where the higher errors occur. While Most errors at located around zero,
some have created a right-skewed tail.

```{r}
# Extract the Out-of-Fold (OOF) predictions from the cross-validated model
oof_predictions <- lm_cv$pred$pred
oof_actual <- lm_cv$pred$obs
oof_errors <- oof_actual - oof_predictions

# Create a data frame for the OOF errors
oof_data <- data.frame(OOF_Error = oof_errors)

# Plot the histogram of OOF errors
ggplot(oof_data, aes(x = OOF_Error)) +
  geom_histogram(binwidth = 10000, fill = "skyblue", color = "black") +
  labs(x = "Mean Absolute Error", y = "Count", title = "Distribution of MAE") +
  theme_minimal()
```

Interpretation

# Goodness of fit metrics

talking about the results of regression text from Fichman paper: The
“MAPE” or mean absolute percentage error, is the absolute value of the
average error (the difference between observed and predicted prices by
tract) represented as a percentage which allows for a more consistent
way to describe model error across cities.

The Standard Deviation of R-Squared measures over-prediction. Using
cross-validation, each time the model is estimated with another set of
randomly drawn observations, we can record goodness of fit. If the model
is truly generalizable to the variation in our Legacy City sample, then
we should expect consistent goodness of fit across each permutation.

If the model is inconsistent across each permutation, it may be that the
goodness of fit is driven solely by individual observations drawn at
random. This latter outcome might indicate overfitting. Thus, this
metric collects R\^2 statistics for each random permutation and then
uses standard deviation to assess whether the variation in goodness of
fit across each permutation is small (ie. generalizability) or a large
(ie. overfitting).

```{r pred}
# MAPE
# Obtain the predicted values from the model:
predicted <- predict(reg2, newdata = dat18.Numeric)

# Calculate the absolute percentage error for each observation:
absolute_error <- abs((dat18.Numeric$Med_House_Val - predicted) / dat18.Numeric$Med_House_Val) * 100

# Mean Absolute Percentage Error (MAPE)
mape <- mean(absolute_error, na.rm = TRUE)

# Residual Standard Error (RSE)
rse <- sqrt(sum(reg2$residuals^2) / reg2$df.residual)

# Adjusted R-squared
adj_r_squared <- summary(reg2)$adj.r.squared

# Create a data frame for the table
results <- data.frame(Model = "reg2",
                       `Residual Standard Error (RSE)` = rse,
                       `Adjusted R-squared` = adj_r_squared,
                       MAPE = mape)

# Print the table
print(results)
```

summary statistics--compare to Fichman paper (lower RSE, MAPE than
Fichman paper)

# Mean Absolute Percentage Error

(x-axis (Observation): This represents the individual observations or
data points in your dataset. Each point on the x-axis corresponds to a
specific data point.

y-axis (MAPE): This represents the Mean Absolute Percentage Error (MAPE)
values. MAPE is a measure of prediction accuracy, typically used for
forecasting models. It calculates the percentage difference between
actual and predicted values.

geom_line(): This function adds a line plot to the graph, connecting the
MAPE values for each observation. This line gives an overview of how
MAPE changes across different observations.

geom_hline(): This function adds a horizontal dashed line at a specific
y-value, which is represented by mape. This line is usually used as a
reference, such as a threshold or target MAPE value.)- for understanding

```{r}
# Create a data frame with MAPE values
mape_data <- data.frame(Observation = 1:length(absolute_error), MAPE = absolute_error)

# Create plot for MAPE
ggplot(mape_data, aes(x = Observation, y = MAPE)) +
  geom_line() +  
  geom_hline(yintercept = mape, linetype = "dashed", color = "red") +  
  labs(x = "Observation", y = "Mean Absolute Percentage Error (MAPE)", title = "MAPE for Linear Regression Model") +
  theme_minimal()
```

Interpretation

# Predicted vs Observed plot

Predicted prices as a function of observed prices for all tracts: If
predictions were perfect, we would expect the below scatterplots to look
like straight lines. most of the points are near the line, except for
one, which is rittenhouse- the model predicts prives poorly for it.

# Outlier is Rittenhouse

```{r predicted Med_Home_Val as a function of observed Med_Home_Val}

plot_data <- data.frame(Observed = dat18.Numeric$Med_House_Val, Predicted = predicted)

# scatter plot
ggplot(plot_data, aes(x = Observed, y = Predicted)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") + 
  labs(x = "Observed Med_Home_Val", y = "Predicted Med_Home_Val", title = "Observed vs Predicted Med_Home_Val") +
  theme_minimal() 

```

# Validating data using Baltimore

# Data assembling

(it took forever!) to explain the process- I combined the data and only
kept the same columns as philly. converted the data into a dataframe
from sf to remove NAs. And added the variable NborHouseVal for spatial
lag that Tao had created for Philly. you can also talk about how the
observations are smaller than Philly which might affect the accuracy.

```{r}
Balti_combined <- rbind(transform(Balti2010, year = 2010), 
                         transform(Balti2014, year = 2014),
                         transform(Balti2018, year = 2018))

Balti_combined <- Balti_combined %>%
  mutate(Total_Pop_2018       = ifelse(year == 2018 & !is.na(Total_Pop), Total_Pop, NA),
    White_Pop_2018       = ifelse(year == 2018 & !is.na(White_Pop), White_Pop, NA),
    BachelorsDegree_2018 = ifelse(year == 2018 & !is.na(BachelorsDegree), BachelorsDegree, NA),
    Med_HH_Inc_2018      = ifelse(year == 2018 & !is.na(Med_HH_Inc), Med_HH_Inc, NA),
    Med_House_Val_2018   = ifelse(year == 2018 & !is.na(Med_House_Val), Med_House_Val, NA),
    Med_Rent_2018        = ifelse(year == 2018 & !is.na(Med_Rent), Med_Rent, NA),
    White_Share_2018     = ifelse(year == 2018 & !is.na(White_Share), White_Share, NA))

Balti_combined <- Balti_combined %>%
  group_by(GEOID, Tract) %>%
  mutate(
    Total_Pop_change = Total_Pop[year == 2014] - Total_Pop[year == 2010],
    Total_White_Pop_change = White_Pop[year == 2014] - White_Pop[year == 2010],
    BachelorsDegree_change = BachelorsDegree[year == 2014] - BachelorsDegree[year == 2010],
    Med_HH_Inc_change = Med_HH_Inc[year == 2014] - Med_HH_Inc[year == 2010],
    Med_House_Val_change = Med_House_Val[year == 2014] - Med_House_Val[year == 2010],
    Med_Rent_change = Med_Rent[year == 2014] - Med_Rent[year == 2010],
    White_Share_Change = White_Share[year == 2014] - White_Share[year == 2010],
    Med_House_Val14 = Med_House_Val[year == 2014],
    Med_House_Val10 = Med_House_Val[year == 2010])

Balti_combined <- Balti_combined %>%
  select(-Total_Pop, -White_Pop, -BachelorsDegree, -Med_HH_Inc,-Med_Rent, -White_Share, -Med_House_Val)


# Convert Balti_combined to a data frame if it's not already
if (!is.data.frame(Balti_combined)) {
  Balti_combined <- as.data.frame(Balti_combined)
}

# Remove rows where Total_Pop_2018 is empty (contains NA values)
Balti_combined <- Balti_combined[complete.cases(Balti_combined$Total_Pop_2018), ]

Balti_combined <- Balti_combined %>% na.omit()

# NborHouseVal
Balti_combinednb <- poly2nb(Balti_combined, queen = TRUE)
Balti_combinedweights <- nb2listw(Balti_combinednb, style = "W")

# Calculate the spatial lag
Balti_combined$NborHouseVal <- lag.listw(Balti_combinedweights, Balti_combined$Med_House_Val10)

```

# Regression

Dropped geometry to run regression. Created regression in a similar way
blah blah blah. Not as strong a association and significance as the
Philly model, however a bigger adjusted r square (cross-check).

-   this is a simplified version of the Philadelphia model, which
    doesn't include some local-level variables but does include a
    spatial lag variable (which would need to be made for each city
    before model application)

```{r}
Balti_combinedNoGeo <- Balti_combined %>%
  st_drop_geometry()
reg3 <- lm(Med_House_Val_2018 ~ . -GEOID -Tract -year -Med_House_Val10 -White_Pop_2018 -White_Share_Change -Total_Pop_change, data = Balti_combinedNoGeo)
summary(reg3)
stargazer(reg2, type = "text")
vif(reg2)
```

# Similar curve- much closerto the line- lesser outlines- predicts exceptionally well

likely fewer extremely high-value neighborhoods

```{r}
# Obtain the predicted values from the model:
Balti_combined$predicted <- predict(reg3, newdata = Balti_combinedNoGeo)

plot_data <- data.frame(Observed = Balti_combined$Med_House_Val_2018, Predicted = Balti_combined$predicted)

# Create scatter plot
ggplot(plot_data, aes(x = Observed, y = Predicted)) +
  geom_point() +  # Add points for observed vs predicted
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red") +  # Add diagonal line for perfect prediction
  labs(x = "Observed Med_Home_Val", y = "Predicted Med_Home_Val", title = "Observed vs Predicted Med_Home_Val") +
  theme_minimal()  # Apply minimal theme
```

# Map

visually illustrates the census tracts- can talked about the few areas
it underpredicts for if you have local knowledge.

```{r fig.height=8, fig.width=8}
a1 <- ggplot()+
  geom_sf(data = Balti_combined, aes(fill = Med_House_Val_2018))+
  scale_fill_viridis_c(labels = scales::label_number(), 
                       name = "Median House Value",
                       na.value = "#ebebeb",
                       guide = guide_colorbar(title.position = "top", title.hjust = 0.5,
                                              title = "Predicted Home Values, In 2010 Dollars"))

a2 <- ggplot()+
  geom_sf(data = Balti_combined, aes(fill = predicted))+
  scale_fill_viridis_c(labels = scales::label_number(), 
                       name = "Median House Value",
                       na.value = "#ebebeb",
                       guide = guide_colorbar(title.position = "top", title.hjust = 0.5,
                                              title = "Predicted Home Values, In 2010 Dollars"))

grid.arrange(a1,a2, ncol =2)
```

# Conclusion

pair your initial idea w/ spatial autocorrelation

talk about model limitations/disclaimers due to differing factors of
gentrification - wouldn't work in Sunbelt, other portions of the country
like Puerto Rico with very different factors driving neighborhood
gentrification and desirability
